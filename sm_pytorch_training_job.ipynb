{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95bb05f-3435-4af2-970b-6a70dffc7ad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2026f267-ad61-4217-ae9b-bcf49ab52c63",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (22.3.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (1.26.45)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3) (0.6.0)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.45 in /opt/conda/lib/python3.7/site-packages (from boto3) (1.29.45)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.45->boto3) (1.26.13)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.45->boto3) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.45->boto3) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.127.0)\n",
      "Requirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.19.6)\n",
      "Requirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.26.45)\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.21.6)\n",
      "Requirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (4.13.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.5)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.3.0)\n",
      "Requirement already satisfied: schema in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.7.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (22.1.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: botocore<1.30.0,>=1.29.45 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.45)\n",
      "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (4.4.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.2)\n",
      "Requirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.7.6.6)\n",
      "Requirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.6)\n",
      "Requirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.2)\n",
      "Requirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.7/site-packages (from schema->sagemaker) (0.6.0.post1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.30.0,>=1.29.45->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torch in /opt/conda/lib/python3.7/site-packages (1.13.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (59.3.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.34.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (0.14.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torchvision) (4.4.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from torchvision) (2.28.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /opt/conda/lib/python3.7/site-packages (from torchvision) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/lib/python3.7/site-packages (from torch==1.13.1->torchvision) (11.7.99)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (0.34.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchvision) (59.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->torchvision) (2.8)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# update boto3 and sagemaker to ensure latest SDK version\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install --upgrade boto3\n",
    "!{sys.executable} -m pip install --upgrade sagemaker\n",
    "!{sys.executable} -m pip install torch\n",
    "!{sys.executable} -m pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0861af1-cda5-48b1-b11f-b90027cf8b60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training-job-experiment-1673148801-5b6e\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.experiments.run import Run\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.utils import unique_name_from_base\n",
    "\n",
    "role = get_execution_role()\n",
    "region = Session().boto_session.region_name\n",
    "\n",
    "\n",
    "# set new experiment configuration\n",
    "experiment_name = unique_name_from_base(\"training-job-experiment\")\n",
    "run_name = \"experiment-run-example\"\n",
    "print(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0770e-6315-4984-bf54-379f0ac54a3c",
   "metadata": {},
   "source": [
    "### Create model training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ac4955f-6a05-4430-9f7e-24615df2319a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "928caeaa-697b-4ec0-9c72-8f19d965eb12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./script/mnist.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/mnist.py\n",
    "# ensure that the latest version of the SageMaker SDK is available\n",
    "import os\n",
    "\n",
    "os.system(\"pip install -U sagemaker\")\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import time\n",
    "from os.path import join\n",
    "import boto3\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.experiments.run import load_run\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "if \"SAGEMAKER_METRICS_DIRECTORY\" in os.environ:\n",
    "    log_file_handler = logging.FileHandler(\n",
    "        join(os.environ[\"SAGEMAKER_METRICS_DIRECTORY\"], \"metrics.json\")\n",
    "    )\n",
    "    formatter = logging.Formatter(\n",
    "        \"{'time':'%(asctime)s', 'name': '%(name)s', \\\n",
    "        'level': '%(levelname)s', 'message': '%(message)s'}\",\n",
    "        style=\"%\",\n",
    "    )\n",
    "    log_file_handler.setFormatter(formatter)\n",
    "    logger.addHandler(log_file_handler)\n",
    "\n",
    "# Based on https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, kernel_size, drop_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(1, hidden_channels, kernel_size=kernel_size)\n",
    "        self.conv2 = torch.nn.Conv2d(hidden_channels, 20, kernel_size=kernel_size)\n",
    "        self.conv2_drop = torch.nn.Dropout2d(p=drop_out)\n",
    "        self.fc1 = torch.nn.Linear(320, 50)\n",
    "        self.fc2 = torch.nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.nn.functional.relu(torch.nn.functional.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.nn.functional.relu(\n",
    "            torch.nn.functional.max_pool2d(self.conv2_drop(self.conv2(x)), 2)\n",
    "        )\n",
    "        x = x.view(-1, 320)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return torch.nn.functional.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "def log_performance(model, data_loader, device, epoch, run, metric_type=\"Test\"):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()  # sum up batch loss\n",
    "            # get the index of the max log-probability\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.0 * correct / len(data_loader.dataset)\n",
    "    # log metrics\n",
    "    run.log_metric(name=metric_type + \":loss\", value=loss, step=epoch)\n",
    "    run.log_metric(name=metric_type + \":accuracy\", value=accuracy, step=epoch)\n",
    "    logger.info(\n",
    "        \"{} Average loss: {:.4f}, {} Accuracy: {:.4f}%;\\n\".format(\n",
    "            metric_type, loss, metric_type, accuracy\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    run, train_set, test_set, data_dir=\"mnist_data\", optimizer=\"sgd\", epochs=10, hidden_channels=10\n",
    "):\n",
    "    \"\"\"\n",
    "    Function that trains the CNN classifier to identify the MNIST digits.\n",
    "    Args:\n",
    "        run (sagemaker.experiments.run.Run): SageMaker Experiment run object\n",
    "        train_set (torchvision.datasets.mnist.MNIST): train dataset\n",
    "        test_set (torchvision.datasets.mnist.MNIST): test dataset\n",
    "        data_dir (str): local directory where the MNIST datasource is stored\n",
    "        optimizer (str): the optimization algorthm to use for training your CNN\n",
    "                         available options are sgd and adam\n",
    "        epochs (int): number of complete pass of the training dataset through the algorithm\n",
    "        hidden_channels (int): number of hidden channels in your model\n",
    "    \"\"\"\n",
    "\n",
    "    # log the parameters of your model\n",
    "    run.log_parameter(\"device\", \"cpu\")\n",
    "    run.log_parameters(\n",
    "        {\n",
    "            \"data_dir\": data_dir,\n",
    "            \"optimizer\": optimizer,\n",
    "            \"epochs\": epochs,\n",
    "            \"hidden_channels\": hidden_channels,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # train the model on the CPU (no GPU)\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    # set the seed for generating random numbers\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "    test_loader = torch.utils.data.DataLoader(test_set, batch_size=1000, shuffle=True)\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of train data\".format(\n",
    "            len(train_loader.sampler),\n",
    "            len(train_loader.dataset),\n",
    "            100.0 * len(train_loader.sampler) / len(train_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        \"Processes {}/{} ({:.0f}%) of test data\".format(\n",
    "            len(test_loader.sampler),\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * len(test_loader.sampler) / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "    model = Net(hidden_channels, kernel_size=5, drop_out=0.5).to(device)\n",
    "    model = torch.nn.DataParallel(model)\n",
    "    momentum = 0.5\n",
    "    lr = 0.01\n",
    "    log_interval = 100\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print(\"Training Epoch:\", epoch)\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = torch.nn.functional.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                logger.info(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)], Train Loss: {:.6f};\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.sampler),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "        log_performance(model, train_loader, device, epoch, run, \"Train\")\n",
    "        log_performance(model, test_loader, device, epoch, run, \"Test\")\n",
    "    # log confusion matrix\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            run.log_confusion_matrix(target, pred, \"Confusion-Matrix-Test-Data\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    hidden_channels = int(os.environ.get(\"hidden_channels\", \"5\"))\n",
    "    kernel_size = int(os.environ.get(\"kernel_size\", \"5\"))\n",
    "    dropout = float(os.environ.get(\"dropout\", \"0.5\"))\n",
    "    model = torch.nn.DataParallel(Net(hidden_channels, kernel_size, dropout))\n",
    "    with open(os.path.join(model_dir, \"model.pth\"), \"rb\") as f:\n",
    "        model.load_state_dict(torch.load(f))\n",
    "        return model.to(device)\n",
    "\n",
    "\n",
    "def save_model(model, model_dir, run):\n",
    "    logger.info(\"Saving the model.\")\n",
    "    path = os.path.join(model_dir, \"model.pth\")\n",
    "    # recommended way from http://pytorch.org/docs/master/notes/serialization.html\n",
    "    torch.save(model.cpu().state_dict(), path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"--epochs\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        metavar=\"N\",\n",
    "        help=\"number of epochs to train (default: 10)\",\n",
    "    )\n",
    "    parser.add_argument(\"--optimizer\", type=str, default=\"sgd\", help=\"optimizer for training.\")\n",
    "    parser.add_argument(\n",
    "        \"--hidden_channels\",\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help=\"number of channels in hidden conv layer\",\n",
    "    )\n",
    "    parser.add_argument(\"--region\", type=str, default=\"us-east-2\", help=\"SageMaker Region\")\n",
    "\n",
    "    # Container environment\n",
    "    parser.add_argument(\"--hosts\", type=list, default=json.loads(os.environ[\"SM_HOSTS\"]))\n",
    "    parser.add_argument(\"--current-host\", type=str, default=os.environ[\"SM_CURRENT_HOST\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--num-gpus\", type=int, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    # download the dataset\n",
    "    # this will not only download data to ./mnist folder, but also load and transform (normalize) them\n",
    "    datasets.MNIST.urls = [\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-images-idx3-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/train-labels-idx1-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-images-idx3-ubyte.gz\",\n",
    "        \"https://sagemaker-sample-files.s3.amazonaws.com/datasets/image/MNIST/t10k-labels-idx1-ubyte.gz\",\n",
    "    ]\n",
    "    train_set = datasets.MNIST(\n",
    "        \"mnist_data\",\n",
    "        train=True,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    test_set = datasets.MNIST(\n",
    "        \"mnist_data\",\n",
    "        train=False,\n",
    "        transform=transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        ),\n",
    "        download=True,\n",
    "    )\n",
    "\n",
    "    session = Session(boto3.session.Session(region_name=args.region))\n",
    "    with load_run(sagemaker_session=session) as run:\n",
    "        run.log_parameters(\n",
    "            {\"num_train_samples\": len(train_set.data), \"num_test_samples\": len(test_set.data)}\n",
    "        )\n",
    "        for f in os.listdir(train_set.raw_folder):\n",
    "            print(\"Logging\", train_set.raw_folder + \"/\" + f)\n",
    "            run.log_file(train_set.raw_folder + \"/\" + f, name=f, is_output=False)\n",
    "        model = train_model(\n",
    "            run,\n",
    "            train_set,\n",
    "            test_set,\n",
    "            data_dir=\"mnist_data\",\n",
    "            optimizer=args.optimizer,\n",
    "            epochs=args.epochs,\n",
    "            hidden_channels=args.hidden_channels,\n",
    "        )\n",
    "        save_model(model, args.model_dir, run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd9ed8da-0b3c-483b-9e56-385e014d7540",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: pytorch-training-2023-01-08-03-40-55-825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-08 03:40:56 Starting - Starting the training job...\n",
      "2023-01-08 03:41:12 Starting - Preparing the instances for training...\n",
      "2023-01-08 03:41:55 Downloading - Downloading input data...\n",
      "2023-01-08 03:42:15 Training - Downloading the training image...\n",
      "2023-01-08 03:42:45 Training - Training image download completed. Training in progress..\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,527 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,529 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,530 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,539 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,541 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,709 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,711 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,722 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,724 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,735 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,737 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:50,746 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"epochs\": 10,\n",
      "        \"hidden_channels\": 5,\n",
      "        \"optimizer\": \"adam\",\n",
      "        \"region\": \"us-west-2\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.c5.xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"job_name\": \"pytorch-training-2023-01-08-03-40-55-825\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-01-08-03-40-55-825/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"mnist\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.c5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.c5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"mnist.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"us-west-2\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=mnist.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.c5.xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=mnist\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-01-08-03-40-55-825/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.c5.xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"epochs\":10,\"hidden_channels\":5,\"optimizer\":\"adam\",\"region\":\"us-west-2\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"job_name\":\"pytorch-training-2023-01-08-03-40-55-825\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-west-2-376678947624/pytorch-training-2023-01-08-03-40-55-825/source/sourcedir.tar.gz\",\"module_name\":\"mnist\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.c5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.c5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"mnist.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--epochs\",\"10\",\"--hidden_channels\",\"5\",\"--optimizer\",\"adam\",\"--region\",\"us-west-2\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=10\u001b[0m\n",
      "\u001b[34mSM_HP_HIDDEN_CHANNELS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OPTIMIZER=adam\u001b[0m\n",
      "\u001b[34mSM_HP_REGION=us-west-2\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.22b20221214-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 mnist.py --epochs 10 --hidden_channels 5 --optimizer adam --region us-west-2\u001b[0m\n",
      "\u001b[34m2023-01-08 03:42:51,283 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sagemaker in /opt/conda/lib/python3.8/site-packages (2.122.0)\u001b[0m\n",
      "\u001b[34mCollecting sagemaker\u001b[0m\n",
      "\u001b[34mDownloading sagemaker-2.127.0.tar.gz (655 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 655.0/655.0 kB 16.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs<23,>=20.3.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: boto3<2.0,>=1.26.28 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.26.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: google-pasta in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0,>=1.9.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.23.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf<4.0,>=3.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (3.19.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf3-to-dict<1.0,>=0.1.5 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: smdebug_rulesconfig==1.0.1 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata<5.0,>=1.4.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (4.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from sagemaker) (22.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.8/site-packages (from sagemaker) (1.5.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pathos in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: schema in /opt/conda/lib/python3.8/site-packages (from sagemaker) (0.7.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (0.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: botocore<1.30.0,>=1.29.30 in /opt/conda/lib/python3.8/site-packages (from boto3<2.0,>=1.26.28->sagemaker) (1.29.30)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata<5.0,>=1.4.0->sagemaker) (3.11.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from protobuf3-to-dict<1.0,>=0.1.5->sagemaker) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2.8.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.8/site-packages (from pandas->sagemaker) (2022.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess>=0.70.14 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.70.14)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pox>=0.3.2 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ppft>=1.7.6.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (1.7.6.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.8/site-packages (from pathos->sagemaker) (0.3.6)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contextlib2>=0.5.5 in /opt/conda/lib/python3.8/site-packages (from schema->sagemaker) (21.6.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.30->boto3<2.0,>=1.26.28->sagemaker) (1.26.13)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for sagemaker (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for sagemaker: filename=sagemaker-2.127.0-py2.py3-none-any.whl size=890080 sha256=329b40d902eca0107178ebf04d721b2967eb71e5d37053771b35d3aa23aeab7e\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/a4/c2/90/d278c752101f59426f774b74361ecfd9ea881f5e3d9c5fd66d\u001b[0m\n",
      "\u001b[34mSuccessfully built sagemaker\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sagemaker\u001b[0m\n",
      "\u001b[34mAttempting uninstall: sagemaker\u001b[0m\n",
      "\u001b[34mFound existing installation: sagemaker 2.122.0\u001b[0m\n",
      "\u001b[34mUninstalling sagemaker-2.122.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled sagemaker-2.122.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed sagemaker-2.127.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/9912422 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 1966080/9912422 [00:00<00:00, 18711694.24it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 4390912/9912422 [00:00<00:00, 21353014.43it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 9912422/9912422 [00:00<00:00, 32698186.35it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/28881 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 28881/28881 [00:00<00:00, 4094773.82it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/1648877 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 1648877/1648877 [00:00<00:00, 23629774.11it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34m0%|          | 0/4542 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 4542/4542 [00:00<00:00, 56196250.05it/s]\u001b[0m\n",
      "\u001b[34mExtracting mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to mnist_data/MNIST/raw\u001b[0m\n",
      "\u001b[34mINFO:sagemaker.experiments.run:The run (experiment-run-example) under experiment (training-job-experiment-1673148801-5b6e) already exists. Loading it. Note: sagemaker.experiments.load_run is recommended to use when the desired run already exists.\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/train-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-images-idx3-ubyte\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\u001b[0m\n",
      "\u001b[34mLogging mnist_data/MNIST/raw/t10k-labels-idx1-ubyte\u001b[0m\n",
      "\u001b[34mProcesses 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 60000/60000 (100%) of train data\u001b[0m\n",
      "\u001b[34mProcesses 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mINFO:__main__:Processes 10000/10000 (100%) of test data\u001b[0m\n",
      "\u001b[34mTraining Epoch: 1\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.016 algo-1:43 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.160 algo-1:43 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.161 algo-1:43 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.161 algo-1:43 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.162 algo-1:43 INFO hook.py:254] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.162 algo-1:43 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.175 algo-1:43 INFO hook.py:561] name:module.conv1.weight count_params:125\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.conv1.bias count_params:5\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.conv2.weight count_params:2500\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.conv2.bias count_params:20\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.fc1.weight count_params:16000\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.fc1.bias count_params:50\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.fc2.weight count_params:500\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:561] name:module.fc2.bias count_params:10\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:563] Total Trainable Params: 19210\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.176 algo-1:43 INFO hook.py:421] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2023-01-08 03:42:59.178 algo-1:43 INFO hook.py:485] Hook is writing from the hook with pid: 43\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [6400/60000 (11%)], Train Loss: 0.619187;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [12800/60000 (21%)], Train Loss: 0.841508;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [19200/60000 (32%)], Train Loss: 0.535673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [25600/60000 (43%)], Train Loss: 0.633673;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [32000/60000 (53%)], Train Loss: 0.604052;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [38400/60000 (64%)], Train Loss: 0.196605;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [44800/60000 (75%)], Train Loss: 0.764071;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [51200/60000 (85%)], Train Loss: 0.366877;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 1 [57600/60000 (96%)], Train Loss: 0.381978;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1360, Train Accuracy: 95.8583%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 2\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1266, Test Accuracy: 96.3100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [6400/60000 (11%)], Train Loss: 0.249367;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [12800/60000 (21%)], Train Loss: 0.260965;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [19200/60000 (32%)], Train Loss: 0.341665;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [25600/60000 (43%)], Train Loss: 0.254568;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [32000/60000 (53%)], Train Loss: 0.625225;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [38400/60000 (64%)], Train Loss: 0.271208;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [44800/60000 (75%)], Train Loss: 0.241313;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [51200/60000 (85%)], Train Loss: 0.258134;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 2 [57600/60000 (96%)], Train Loss: 0.276261;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1211, Train Accuracy: 96.2967%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 3\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.4100%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [6400/60000 (11%)], Train Loss: 0.540804;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [12800/60000 (21%)], Train Loss: 0.507104;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [19200/60000 (32%)], Train Loss: 0.732960;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [25600/60000 (43%)], Train Loss: 0.478505;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [32000/60000 (53%)], Train Loss: 0.201122;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [38400/60000 (64%)], Train Loss: 0.261213;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [44800/60000 (75%)], Train Loss: 0.312363;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [51200/60000 (85%)], Train Loss: 0.468251;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 3 [57600/60000 (96%)], Train Loss: 0.477161;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1195, Train Accuracy: 96.5067%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 4\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1105, Test Accuracy: 96.7500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [6400/60000 (11%)], Train Loss: 0.333059;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [12800/60000 (21%)], Train Loss: 0.165329;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [19200/60000 (32%)], Train Loss: 0.313524;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [25600/60000 (43%)], Train Loss: 0.170249;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [32000/60000 (53%)], Train Loss: 0.286480;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [38400/60000 (64%)], Train Loss: 0.191275;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [44800/60000 (75%)], Train Loss: 0.423735;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [51200/60000 (85%)], Train Loss: 0.274153;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 4 [57600/60000 (96%)], Train Loss: 0.593129;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1136, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 5\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1061, Test Accuracy: 96.7700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [6400/60000 (11%)], Train Loss: 0.343393;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [12800/60000 (21%)], Train Loss: 0.390378;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [19200/60000 (32%)], Train Loss: 0.360940;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [25600/60000 (43%)], Train Loss: 0.489606;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [32000/60000 (53%)], Train Loss: 0.268219;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [38400/60000 (64%)], Train Loss: 0.237594;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [44800/60000 (75%)], Train Loss: 0.415231;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [51200/60000 (85%)], Train Loss: 0.176550;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 5 [57600/60000 (96%)], Train Loss: 0.283533;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1177, Train Accuracy: 96.5850%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch:\u001b[0m\n",
      "\u001b[34m6\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1108, Test Accuracy: 96.8700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [6400/60000 (11%)], Train Loss: 0.270528;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [12800/60000 (21%)], Train Loss: 0.321458;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [19200/60000 (32%)], Train Loss: 0.289076;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [25600/60000 (43%)], Train Loss: 0.365472;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [32000/60000 (53%)], Train Loss: 0.418724;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [38400/60000 (64%)], Train Loss: 0.341385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [44800/60000 (75%)], Train Loss: 0.121961;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [51200/60000 (85%)], Train Loss: 0.361106;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 6 [57600/60000 (96%)], Train Loss: 0.265346;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1122, Train Accuracy: 96.5333%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 7\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1090, Test Accuracy: 96.6700%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [6400/60000 (11%)], Train Loss: 0.264878;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [12800/60000 (21%)], Train Loss: 0.377314;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [19200/60000 (32%)], Train Loss: 0.282708;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [25600/60000 (43%)], Train Loss: 0.270151;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [32000/60000 (53%)], Train Loss: 0.463462;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [38400/60000 (64%)], Train Loss: 0.318820;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [44800/60000 (75%)], Train Loss: 0.301752;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [51200/60000 (85%)], Train Loss: 0.491330;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 7 [57600/60000 (96%)], Train Loss: 0.605309;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1232, Train Accuracy: 96.2983%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 8\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1162, Test Accuracy: 96.6500%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [6400/60000 (11%)], Train Loss: 0.436534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [12800/60000 (21%)], Train Loss: 0.697182;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [19200/60000 (32%)], Train Loss: 0.352988;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [25600/60000 (43%)], Train Loss: 0.251108;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [32000/60000 (53%)], Train Loss: 0.337925;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [38400/60000 (64%)], Train Loss: 0.243655;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [44800/60000 (75%)], Train Loss: 0.260882;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [51200/60000 (85%)], Train Loss: 0.439534;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 8 [57600/60000 (96%)], Train Loss: 0.307441;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1223, Train Accuracy: 96.1400%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 9\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1160, Test Accuracy: 96.4200%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [6400/60000 (11%)], Train Loss: 0.541070;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [12800/60000 (21%)], Train Loss: 0.434201;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [19200/60000 (32%)], Train Loss: 0.197400;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [25600/60000 (43%)], Train Loss: 0.469770;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [32000/60000 (53%)], Train Loss: 0.159525;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [38400/60000 (64%)], Train Loss: 0.757667;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [44800/60000 (75%)], Train Loss: 0.284950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [51200/60000 (85%)], Train Loss: 0.334385;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 9 [57600/60000 (96%)], Train Loss: 0.203318;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1043, Train Accuracy: 96.9667%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTraining Epoch: 10\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1033, Test Accuracy: 97.1400%;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [6400/60000 (11%)], Train Loss: 0.277690;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [12800/60000 (21%)], Train Loss: 0.492950;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [19200/60000 (32%)], Train Loss: 0.284791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [25600/60000 (43%)], Train Loss: 0.307791;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [32000/60000 (53%)], Train Loss: 0.591849;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [38400/60000 (64%)], Train Loss: 0.527022;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [44800/60000 (75%)], Train Loss: 0.192943;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [51200/60000 (85%)], Train Loss: 0.461220;\u001b[0m\n",
      "\u001b[34mTrain Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Epoch: 10 [57600/60000 (96%)], Train Loss: 0.206218;\u001b[0m\n",
      "\u001b[34mTrain Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Train Average loss: 0.1205, Train Accuracy: 96.3250%;\u001b[0m\n",
      "\u001b[34mTest Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mINFO:__main__:Test Average loss: 0.1264, Test Accuracy: 96.4600%;\u001b[0m\n",
      "\u001b[34mSaving the model.\u001b[0m\n",
      "\u001b[34mINFO:__main__:Saving the model.\u001b[0m\n",
      "\u001b[34m2023-01-08 03:47:17,667 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2023-01-08 03:47:17,667 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2023-01-08 03:47:17,668 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2023-01-08 03:47:35 Uploading - Uploading generated training model\n",
      "2023-01-08 03:47:35 Completed - Instances not retained as a result of warmpool resource limits being exceeded\n",
      "Training seconds: 339\n",
      "Billable seconds: 339\n",
      "CPU times: user 1.1 s, sys: 113 ms, total: 1.22 s\n",
      "Wall time: 7min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Start training job with experiment setting\n",
    "with Run(experiment_name=experiment_name, run_name=run_name, sagemaker_session=Session()) as run:\n",
    "    est = PyTorch(\n",
    "        entry_point=\"./script/mnist.py\",\n",
    "        role=role,\n",
    "        model_dir=False,\n",
    "        framework_version=\"1.12\",\n",
    "        py_version=\"py38\",\n",
    "        instance_type=\"ml.c5.xlarge\",\n",
    "        instance_count=1,\n",
    "        hyperparameters={\"epochs\": 10, \"hidden_channels\": 5, \"optimizer\": \"adam\", \"region\": region},\n",
    "        keep_alive_period_in_seconds=3600,\n",
    "    )\n",
    "\n",
    "    est.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b117a2-4298-44e4-a583-dd5e36c1b556",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
